{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Generative Image Inpainting.ipynb","provenance":[{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1596153089372},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YlfaeL-0Itkb","executionInfo":{"status":"ok","timestamp":1596153171201,"user_tz":300,"elapsed":5731,"user":{"displayName":"ANDY GIANPIERO NACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"e668fb55-c7bd-4aa1-b81e-8f6d5b6e8305","colab":{"base_uri":"https://localhost:8080/","height":357}},"source":["!nvidia-smi\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Jul 30 23:52:47 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qex2EbDjJIy6"},"source":["DESCARGAR DATASET\n","\n"]},{"cell_type":"code","metadata":{"id":"ZV6_E6IoJvzD","executionInfo":{"status":"ok","timestamp":1596203219092,"user_tz":300,"elapsed":639496,"user":{"displayName":"ANDY GIANPIERO NACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"e2d55bbb-2c12-428b-deda-2d3eb95f345f","colab":{"base_uri":"https://localhost:8080/","height":272}},"source":["!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip"],"execution_count":null,"outputs":[{"output_type":"stream","text":["--2020-07-31 13:36:22--  http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n","Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.162\n","Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:80... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip [following]\n","--2020-07-31 13:36:23--  https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\n","Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3530603713 (3.3G) [application/zip]\n","Saving to: ‘DIV2K_train_HR.zip’\n","\n","DIV2K_train_HR.zip  100%[===================>]   3.29G  6.36MB/s    in 10m 31s \n","\n","2020-07-31 13:46:56 (5.33 MB/s) - ‘DIV2K_train_HR.zip’ saved [3530603713/3530603713]\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QDD8z2nmJPel"},"source":["CONECTAR CON GOOGLE DRIVE"]},{"cell_type":"code","metadata":{"id":"T8Sds2_sKsIg","executionInfo":{"status":"ok","timestamp":1596203247615,"user_tz":300,"elapsed":27027,"user":{"displayName":"ANDY GIANPIERO NACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"6f17ccf0-fc29-4e85-b1b0-6723487035b4","colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NMDbbAcvMBJ-","executionInfo":{"status":"ok","timestamp":1596203396670,"user_tz":300,"elapsed":64300,"user":{"displayName":"ANDY GIANPIERO NACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"d8c077fb-b302-4642-ee08-40586d95adc3","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["!ls\n","shutil.move(\"/content/DIV2K_train_HR.zip\",\"/content/drive/My Drive/7 semestre/Investigacion En CC/SRGAN\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["DIV2K_train_HR.zip  drive  sample_data\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/7 semestre/Investigacion En CC/SRGAN/DIV2K_train_HR.zip'"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"OeIcT6GkIin4","executionInfo":{"status":"ok","timestamp":1596203459913,"user_tz":300,"elapsed":4632,"user":{"displayName":"ANDY GIANPIERO NACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"1977826c-bbf6-4d55-eca4-1b98fa5f733a","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["os.chdir(\"/content/drive/My Drive/7 semestre/Investigacion En CC/SRGAN\")\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["DIV2K_train_HR.zip\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"m2CvBh43K6ai","executionInfo":{"status":"ok","timestamp":1596203297237,"user_tz":300,"elapsed":3738,"user":{"displayName":"ANDY GIANPIERO NACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"5091fc18-3dda-4ced-b901-3d4f6fc96f5f","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import  os,shutil\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tqdm.notebook import tqdm                \n","from keras.optimizers import Adam\n","from keras.applications import VGG19\n","from keras.callbacks import ReduceLROnPlateau"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"Rrt1tZJiK9dT"},"source":["!mkdir train_data\n","shutil.move('./DIV2K_train_HR.zip','./train_data')\n","os.chdir('./train_data')\n","!unzip DIV2K_train_HR.zip\n","os.chdir('./DIV2K_train_HR')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VvrQRnhGSh2t","executionInfo":{"status":"ok","timestamp":1605181790892,"user_tz":300,"elapsed":2637,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"ffa55ff5-5f19-4888-b694-60538495e7f9","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git clone https://github.com/JiahuiYu/generative_inpainting"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Cloning into 'generative_inpainting'...\n","remote: Enumerating objects: 193, done.\u001b[K\n","remote: Total 193 (delta 0), reused 0 (delta 0), pack-reused 193\u001b[K\n","Receiving objects: 100% (193/193), 13.77 MiB | 52.01 MiB/s, done.\n","Resolving deltas: 100% (63/63), done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Knj5MQ1TSk2X","executionInfo":{"status":"ok","timestamp":1605182021001,"user_tz":300,"elapsed":4316,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"b9d7a797-5a1f-4430-9f72-e3d6db452594","colab":{"base_uri":"https://localhost:8080/"}},"source":["pip install git+https://github.com/JiahuiYu/neuralgym\n","%tensorflow_version 1.x"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting git+https://github.com/JiahuiYu/neuralgym\n","  Cloning https://github.com/JiahuiYu/neuralgym to /tmp/pip-req-build-vr2xxhzo\n","  Running command git clone -q https://github.com/JiahuiYu/neuralgym /tmp/pip-req-build-vr2xxhzo\n","Requirement already satisfied (use --upgrade to upgrade): neuralgym==0.0.1 from git+https://github.com/JiahuiYu/neuralgym in /usr/local/lib/python3.6/dist-packages\n","Building wheels for collected packages: neuralgym\n","  Building wheel for neuralgym (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for neuralgym: filename=neuralgym-0.0.1-cp36-none-any.whl size=40428 sha256=c6a3a944edd753c97ce5722db6ec79556764e4d1b60185eb2c68b5d5bfd76f2f\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-0s5kunyy/wheels/bc/dd/bc/fbaacf774dfc69223a01b3e037b8539f24318193a37221bb3b\n","Successfully built neuralgym\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fd4hgGUNW_GH"},"source":["import os\n","os.chdir('generative_inpainting')\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pDin3_Y3VDB-"},"source":["IMPAINT_OPS"]},{"cell_type":"code","metadata":{"id":"4FMgCQMpTpgQ","executionInfo":{"status":"ok","timestamp":1605182314499,"user_tz":300,"elapsed":3148,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"5fb45e79-a749-4ea9-8159-3fc434dd7bd4","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["import logging\n","import math\n","\n","import cv2\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.contrib.framework.python.ops import add_arg_scope\n","from PIL import Image, ImageDraw\n","\n","from neuralgym.ops.layers import resize\n","from neuralgym.ops.layers import *\n","from neuralgym.ops.loss_ops import *\n","from neuralgym.ops.gan_ops import *\n","from neuralgym.ops.summary_ops import *\n","\n","\n","logger = logging.getLogger()\n","np.random.seed(2018)\n","\n","\n","@add_arg_scope\n","def gen_conv(x, cnum, ksize, stride=1, rate=1, name='conv',\n","             padding='SAME', activation=tf.nn.elu, training=True):\n","    \"\"\"Define conv for generator.\n","\n","    Args:\n","        x: Input.\n","        cnum: Channel number.\n","        ksize: Kernel size.\n","        Stride: Convolution stride.\n","        Rate: Rate for or dilated conv.\n","        name: Name of layers.\n","        padding: Default to SYMMETRIC.\n","        activation: Activation function after convolution.\n","        training: If current graph is for training or inference, used for bn.\n","\n","    Returns:\n","        tf.Tensor: output\n","\n","    \"\"\"\n","    assert padding in ['SYMMETRIC', 'SAME', 'REFELECT']\n","    if padding == 'SYMMETRIC' or padding == 'REFELECT':\n","        p = int(rate*(ksize-1)/2)\n","        x = tf.pad(x, [[0,0], [p, p], [p, p], [0,0]], mode=padding)\n","        padding = 'VALID'\n","    x = tf.layers.conv2d(\n","        x, cnum, ksize, stride, dilation_rate=rate,\n","        activation=None, padding=padding, name=name)\n","    if cnum == 3 or activation is None:\n","        # conv for output\n","        return x\n","    x, y = tf.split(x, 2, 3)\n","    x = activation(x)\n","    y = tf.nn.sigmoid(y)\n","    x = x * y\n","    return x\n","\n","\n","@add_arg_scope\n","def gen_deconv(x, cnum, name='upsample', padding='SAME', training=True):\n","    \"\"\"Define deconv for generator.\n","    The deconv is defined to be a x2 resize_nearest_neighbor operation with\n","    additional gen_conv operation.\n","\n","    Args:\n","        x: Input.\n","        cnum: Channel number.\n","        name: Name of layers.\n","        training: If current graph is for training or inference, used for bn.\n","\n","    Returns:\n","        tf.Tensor: output\n","\n","    \"\"\"\n","    with tf.variable_scope(name):\n","        x = resize(x, func=tf.image.resize_nearest_neighbor)\n","        x = gen_conv(\n","            x, cnum, 3, 1, name=name+'_conv', padding=padding,\n","            training=training)\n","    return x\n","\n","\n","@add_arg_scope\n","def dis_conv(x, cnum, ksize=5, stride=2, name='conv', training=True):\n","    \"\"\"Define conv for discriminator.\n","    Activation is set to leaky_relu.\n","\n","    Args:\n","        x: Input.\n","        cnum: Channel number.\n","        ksize: Kernel size.\n","        Stride: Convolution stride.\n","        name: Name of layers.\n","        training: If current graph is for training or inference, used for bn.\n","\n","    Returns:\n","        tf.Tensor: output\n","\n","    \"\"\"\n","    x = conv2d_spectral_norm(x, cnum, ksize, stride, 'SAME', name=name)\n","    x = tf.nn.leaky_relu(x)\n","    return x\n","\n","\n","def random_bbox(FLAGS):\n","    \"\"\"Generate a random tlhw.\n","\n","    Returns:\n","        tuple: (top, left, height, width)\n","\n","    \"\"\"\n","    img_shape = FLAGS.img_shapes\n","    img_height = img_shape[0]\n","    img_width = img_shape[1]\n","    maxt = img_height - FLAGS.vertical_margin - FLAGS.height\n","    maxl = img_width - FLAGS.horizontal_margin - FLAGS.width\n","    t = tf.random_uniform(\n","        [], minval=FLAGS.vertical_margin, maxval=maxt, dtype=tf.int32)\n","    l = tf.random_uniform(\n","        [], minval=FLAGS.horizontal_margin, maxval=maxl, dtype=tf.int32)\n","    h = tf.constant(FLAGS.height)\n","    w = tf.constant(FLAGS.width)\n","    return (t, l, h, w)\n","\n","\n","def bbox2mask(FLAGS, bbox, name='mask'):\n","    \"\"\"Generate mask tensor from bbox.\n","\n","    Args:\n","        bbox: tuple, (top, left, height, width)\n","\n","    Returns:\n","        tf.Tensor: output with shape [1, H, W, 1]\n","\n","    \"\"\"\n","    def npmask(bbox, height, width, delta_h, delta_w):\n","        mask = np.zeros((1, height, width, 1), np.float32)\n","        h = np.random.randint(delta_h//2+1)\n","        w = np.random.randint(delta_w//2+1)\n","        mask[:, bbox[0]+h:bbox[0]+bbox[2]-h,\n","             bbox[1]+w:bbox[1]+bbox[3]-w, :] = 1.\n","        return mask\n","    with tf.variable_scope(name), tf.device('/cpu:0'):\n","        img_shape = FLAGS.img_shapes\n","        height = img_shape[0]\n","        width = img_shape[1]\n","        mask = tf.py_func(\n","            npmask,\n","            [bbox, height, width,\n","             FLAGS.max_delta_height, FLAGS.max_delta_width],\n","            tf.float32, stateful=False)\n","        mask.set_shape([1] + [height, width] + [1])\n","    return mask\n","\n","\n","def brush_stroke_mask(FLAGS, name='mask'):\n","    \"\"\"Generate mask tensor from bbox.\n","\n","    Returns:\n","        tf.Tensor: output with shape [1, H, W, 1]\n","\n","    \"\"\"\n","    min_num_vertex = 4\n","    max_num_vertex = 12\n","    mean_angle = 2*math.pi / 5\n","    angle_range = 2*math.pi / 15\n","    min_width = 12\n","    max_width = 40\n","    def generate_mask(H, W):\n","        average_radius = math.sqrt(H*H+W*W) / 8\n","        mask = Image.new('L', (W, H), 0)\n","\n","        for _ in range(np.random.randint(1, 4)):\n","            num_vertex = np.random.randint(min_num_vertex, max_num_vertex)\n","            angle_min = mean_angle - np.random.uniform(0, angle_range)\n","            angle_max = mean_angle + np.random.uniform(0, angle_range)\n","            angles = []\n","            vertex = []\n","            for i in range(num_vertex):\n","                if i % 2 == 0:\n","                    angles.append(2*math.pi - np.random.uniform(angle_min, angle_max))\n","                else:\n","                    angles.append(np.random.uniform(angle_min, angle_max))\n","\n","            h, w = mask.size\n","            vertex.append((int(np.random.randint(0, w)), int(np.random.randint(0, h))))\n","            for i in range(num_vertex):\n","                r = np.clip(\n","                    np.random.normal(loc=average_radius, scale=average_radius//2),\n","                    0, 2*average_radius)\n","                new_x = np.clip(vertex[-1][0] + r * math.cos(angles[i]), 0, w)\n","                new_y = np.clip(vertex[-1][1] + r * math.sin(angles[i]), 0, h)\n","                vertex.append((int(new_x), int(new_y)))\n","\n","            draw = ImageDraw.Draw(mask)\n","            width = int(np.random.uniform(min_width, max_width))\n","            draw.line(vertex, fill=1, width=width)\n","            for v in vertex:\n","                draw.ellipse((v[0] - width//2,\n","                              v[1] - width//2,\n","                              v[0] + width//2,\n","                              v[1] + width//2),\n","                             fill=1)\n","\n","        if np.random.normal() > 0:\n","            mask.transpose(Image.FLIP_LEFT_RIGHT)\n","        if np.random.normal() > 0:\n","            mask.transpose(Image.FLIP_TOP_BOTTOM)\n","        mask = np.asarray(mask, np.float32)\n","        mask = np.reshape(mask, (1, H, W, 1))\n","        return mask\n","    with tf.variable_scope(name), tf.device('/cpu:0'):\n","        img_shape = FLAGS.img_shapes\n","        height = img_shape[0]\n","        width = img_shape[1]\n","        mask = tf.py_func(\n","            generate_mask,\n","            [height, width],\n","            tf.float32, stateful=True)\n","        mask.set_shape([1] + [height, width] + [1])\n","    return mask\n","\n","\n","def local_patch(x, bbox):\n","    \"\"\"Crop local patch according to bbox.\n","\n","    Args:\n","        x: input\n","        bbox: (top, left, height, width)\n","\n","    Returns:\n","        tf.Tensor: local patch\n","\n","    \"\"\"\n","    x = tf.image.crop_to_bounding_box(x, bbox[0], bbox[1], bbox[2], bbox[3])\n","    return x\n","\n","\n","def resize_mask_like(mask, x):\n","    \"\"\"Resize mask like shape of x.\n","\n","    Args:\n","        mask: Original mask.\n","        x: To shape of x.\n","\n","    Returns:\n","        tf.Tensor: resized mask\n","\n","    \"\"\"\n","    mask_resize = resize(\n","        mask, to_shape=x.get_shape().as_list()[1:3],\n","        func=tf.image.resize_nearest_neighbor)\n","    return mask_resize\n","\n","\n","def contextual_attention(f, b, mask=None, ksize=3, stride=1, rate=1,\n","                         fuse_k=3, softmax_scale=10., training=True, fuse=True):\n","    \"\"\" Contextual attention layer implementation.\n","\n","    Contextual attention is first introduced in publication:\n","        Generative Image Inpainting with Contextual Attention, Yu et al.\n","\n","    Args:\n","        x: Input feature to match (foreground).\n","        t: Input feature for match (background).\n","        mask: Input mask for t, indicating patches not available.\n","        ksize: Kernel size for contextual attention.\n","        stride: Stride for extracting patches from t.\n","        rate: Dilation for matching.\n","        softmax_scale: Scaled softmax for attention.\n","        training: Indicating if current graph is training or inference.\n","\n","    Returns:\n","        tf.Tensor: output\n","\n","    \"\"\"\n","    # get shapes\n","    raw_fs = tf.shape(f)\n","    raw_int_fs = f.get_shape().as_list()\n","    raw_int_bs = b.get_shape().as_list()\n","    # extract patches from background with stride and rate\n","    kernel = 2*rate\n","    raw_w = tf.extract_image_patches(\n","        b, [1,kernel,kernel,1], [1,rate*stride,rate*stride,1], [1,1,1,1], padding='SAME')\n","    raw_w = tf.reshape(raw_w, [raw_int_bs[0], -1, kernel, kernel, raw_int_bs[3]])\n","    raw_w = tf.transpose(raw_w, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw\n","    # downscaling foreground option: downscaling both foreground and\n","    # background for matching and use original background for reconstruction.\n","    f = resize(f, scale=1./rate, func=tf.image.resize_nearest_neighbor)\n","    b = resize(b, to_shape=[int(raw_int_bs[1]/rate), int(raw_int_bs[2]/rate)], func=tf.image.resize_nearest_neighbor)  # https://github.com/tensorflow/tensorflow/issues/11651\n","    if mask is not None:\n","        mask = resize(mask, scale=1./rate, func=tf.image.resize_nearest_neighbor)\n","    fs = tf.shape(f)\n","    int_fs = f.get_shape().as_list()\n","    f_groups = tf.split(f, int_fs[0], axis=0)\n","    # from t(H*W*C) to w(b*k*k*c*h*w)\n","    bs = tf.shape(b)\n","    int_bs = b.get_shape().as_list()\n","    w = tf.extract_image_patches(\n","        b, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')\n","    w = tf.reshape(w, [int_fs[0], -1, ksize, ksize, int_fs[3]])\n","    w = tf.transpose(w, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw\n","    # process mask\n","    if mask is None:\n","        mask = tf.zeros([1, bs[1], bs[2], 1])\n","    m = tf.extract_image_patches(\n","        mask, [1,ksize,ksize,1], [1,stride,stride,1], [1,1,1,1], padding='SAME')\n","    m = tf.reshape(m, [1, -1, ksize, ksize, 1])\n","    m = tf.transpose(m, [0, 2, 3, 4, 1])  # transpose to b*k*k*c*hw\n","    m = m[0]\n","    mm = tf.cast(tf.equal(tf.reduce_mean(m, axis=[0,1,2], keep_dims=True), 0.), tf.float32)\n","    w_groups = tf.split(w, int_bs[0], axis=0)\n","    raw_w_groups = tf.split(raw_w, int_bs[0], axis=0)\n","    y = []\n","    offsets = []\n","    k = fuse_k\n","    scale = softmax_scale\n","    fuse_weight = tf.reshape(tf.eye(k), [k, k, 1, 1])\n","    for xi, wi, raw_wi in zip(f_groups, w_groups, raw_w_groups):\n","        # conv for compare\n","        wi = wi[0]\n","        wi_normed = wi / tf.maximum(tf.sqrt(tf.reduce_sum(tf.square(wi), axis=[0,1,2])), 1e-4)\n","        yi = tf.nn.conv2d(xi, wi_normed, strides=[1,1,1,1], padding=\"SAME\")\n","\n","        # conv implementation for fuse scores to encourage large patches\n","        if fuse:\n","            yi = tf.reshape(yi, [1, fs[1]*fs[2], bs[1]*bs[2], 1])\n","            yi = tf.nn.conv2d(yi, fuse_weight, strides=[1,1,1,1], padding='SAME')\n","            yi = tf.reshape(yi, [1, fs[1], fs[2], bs[1], bs[2]])\n","            yi = tf.transpose(yi, [0, 2, 1, 4, 3])\n","            yi = tf.reshape(yi, [1, fs[1]*fs[2], bs[1]*bs[2], 1])\n","            yi = tf.nn.conv2d(yi, fuse_weight, strides=[1,1,1,1], padding='SAME')\n","            yi = tf.reshape(yi, [1, fs[2], fs[1], bs[2], bs[1]])\n","            yi = tf.transpose(yi, [0, 2, 1, 4, 3])\n","        yi = tf.reshape(yi, [1, fs[1], fs[2], bs[1]*bs[2]])\n","\n","        # softmax to match\n","        yi *=  mm  # mask\n","        yi = tf.nn.softmax(yi*scale, 3)\n","        yi *=  mm  # mask\n","\n","        offset = tf.argmax(yi, axis=3, output_type=tf.int32)\n","        offset = tf.stack([offset // fs[2], offset % fs[2]], axis=-1)\n","        # deconv for patch pasting\n","        # 3.1 paste center\n","        wi_center = raw_wi[0]\n","        yi = tf.nn.conv2d_transpose(yi, wi_center, tf.concat([[1], raw_fs[1:]], axis=0), strides=[1,rate,rate,1]) / 4.\n","        y.append(yi)\n","        offsets.append(offset)\n","    y = tf.concat(y, axis=0)\n","    y.set_shape(raw_int_fs)\n","    offsets = tf.concat(offsets, axis=0)\n","    offsets.set_shape(int_bs[:3] + [2])\n","    # case1: visualize optical flow: minus current position\n","    h_add = tf.tile(tf.reshape(tf.range(bs[1]), [1, bs[1], 1, 1]), [bs[0], 1, bs[2], 1])\n","    w_add = tf.tile(tf.reshape(tf.range(bs[2]), [1, 1, bs[2], 1]), [bs[0], bs[1], 1, 1])\n","    offsets = offsets - tf.concat([h_add, w_add], axis=3)\n","    # to flow image\n","    flow = flow_to_image_tf(offsets)\n","    # # case2: visualize which pixels are attended\n","    # flow = highlight_flow_tf(offsets * tf.cast(mask, tf.int32))\n","    if rate != 1:\n","        flow = resize(flow, scale=rate, func=tf.image.resize_bilinear)\n","    return y, flow\n","\n","\n","def test_contextual_attention(args):\n","    \"\"\"Test contextual attention layer with 3-channel image input\n","    (instead of n-channel feature).\n","\n","    \"\"\"\n","    import cv2\n","    import os\n","    # run on cpu\n","    os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n","\n","    rate = 2\n","    stride = 1\n","    grid = rate*stride\n","\n","    b = cv2.imread(args.imageA)\n","    b = cv2.resize(b, None, fx=0.5, fy=0.5, interpolation=cv2.INTER_CUBIC)\n","    h, w, _ = b.shape\n","    b = b[:h//grid*grid, :w//grid*grid, :]\n","    b = np.expand_dims(b, 0)\n","    logger.info('Size of imageA: {}'.format(b.shape))\n","\n","    f = cv2.imread(args.imageB)\n","    h, w, _ = f.shape\n","    f = f[:h//grid*grid, :w//grid*grid, :]\n","    f = np.expand_dims(f, 0)\n","    logger.info('Size of imageB: {}'.format(f.shape))\n","\n","    with tf.Session() as sess:\n","        bt = tf.constant(b, dtype=tf.float32)\n","        ft = tf.constant(f, dtype=tf.float32)\n","\n","        yt, flow = contextual_attention(\n","            ft, bt, stride=stride, rate=rate,\n","            training=False, fuse=False)\n","        y = sess.run(yt)\n","        cv2.imwrite(args.imageOut, y[0])\n","\n","\n","def make_color_wheel():\n","    RY, YG, GC, CB, BM, MR = (15, 6, 4, 11, 13, 6)\n","    ncols = RY + YG + GC + CB + BM + MR\n","    colorwheel = np.zeros([ncols, 3])\n","    col = 0\n","    # RY\n","    colorwheel[0:RY, 0] = 255\n","    colorwheel[0:RY, 1] = np.transpose(np.floor(255*np.arange(0, RY) / RY))\n","    col += RY\n","    # YG\n","    colorwheel[col:col+YG, 0] = 255 - np.transpose(np.floor(255*np.arange(0, YG) / YG))\n","    colorwheel[col:col+YG, 1] = 255\n","    col += YG\n","    # GC\n","    colorwheel[col:col+GC, 1] = 255\n","    colorwheel[col:col+GC, 2] = np.transpose(np.floor(255*np.arange(0, GC) / GC))\n","    col += GC\n","    # CB\n","    colorwheel[col:col+CB, 1] = 255 - np.transpose(np.floor(255*np.arange(0, CB) / CB))\n","    colorwheel[col:col+CB, 2] = 255\n","    col += CB\n","    # BM\n","    colorwheel[col:col+BM, 2] = 255\n","    colorwheel[col:col+BM, 0] = np.transpose(np.floor(255*np.arange(0, BM) / BM))\n","    col += + BM\n","    # MR\n","    colorwheel[col:col+MR, 2] = 255 - np.transpose(np.floor(255 * np.arange(0, MR) / MR))\n","    colorwheel[col:col+MR, 0] = 255\n","    return colorwheel\n","\n","\n","COLORWHEEL = make_color_wheel()\n","\n","\n","def compute_color(u,v):\n","    h, w = u.shape\n","    img = np.zeros([h, w, 3])\n","    nanIdx = np.isnan(u) | np.isnan(v)\n","    u[nanIdx] = 0\n","    v[nanIdx] = 0\n","    # colorwheel = COLORWHEEL\n","    colorwheel = make_color_wheel()\n","    ncols = np.size(colorwheel, 0)\n","    rad = np.sqrt(u**2+v**2)\n","    a = np.arctan2(-v, -u) / np.pi\n","    fk = (a+1) / 2 * (ncols - 1) + 1\n","    k0 = np.floor(fk).astype(int)\n","    k1 = k0 + 1\n","    k1[k1 == ncols+1] = 1\n","    f = fk - k0\n","    for i in range(np.size(colorwheel,1)):\n","        tmp = colorwheel[:, i]\n","        col0 = tmp[k0-1] / 255\n","        col1 = tmp[k1-1] / 255\n","        col = (1-f) * col0 + f * col1\n","        idx = rad <= 1\n","        col[idx] = 1-rad[idx]*(1-col[idx])\n","        notidx = np.logical_not(idx)\n","        col[notidx] *= 0.75\n","        img[:, :, i] = np.uint8(np.floor(255 * col*(1-nanIdx)))\n","    return img\n","\n","\n","\n","def flow_to_image(flow):\n","    \"\"\"Transfer flow map to image.\n","    Part of code forked from flownet.\n","    \"\"\"\n","    out = []\n","    maxu = -999.\n","    maxv = -999.\n","    minu = 999.\n","    minv = 999.\n","    maxrad = -1\n","    for i in range(flow.shape[0]):\n","        u = flow[i, :, :, 0]\n","        v = flow[i, :, :, 1]\n","        idxunknow = (abs(u) > 1e7) | (abs(v) > 1e7)\n","        u[idxunknow] = 0\n","        v[idxunknow] = 0\n","        maxu = max(maxu, np.max(u))\n","        minu = min(minu, np.min(u))\n","        maxv = max(maxv, np.max(v))\n","        minv = min(minv, np.min(v))\n","        rad = np.sqrt(u ** 2 + v ** 2)\n","        maxrad = max(maxrad, np.max(rad))\n","        u = u/(maxrad + np.finfo(float).eps)\n","        v = v/(maxrad + np.finfo(float).eps)\n","        img = compute_color(u, v)\n","        out.append(img)\n","    return np.float32(np.uint8(out))\n","\n","\n","def flow_to_image_tf(flow, name='flow_to_image'):\n","    \"\"\"Tensorflow ops for computing flow to image.\n","    \"\"\"\n","    with tf.variable_scope(name), tf.device('/cpu:0'):\n","        img = tf.py_func(flow_to_image, [flow], tf.float32, stateful=False)\n","        img.set_shape(flow.get_shape().as_list()[0:-1]+[3])\n","        img = img / 127.5 - 1.\n","        return img\n","\n","\n","def highlight_flow(flow):\n","    \"\"\"Convert flow into middlebury color code image.\n","    \"\"\"\n","    out = []\n","    s = flow.shape\n","    for i in range(flow.shape[0]):\n","        img = np.ones((s[1], s[2], 3)) * 144.\n","        u = flow[i, :, :, 0]\n","        v = flow[i, :, :, 1]\n","        for h in range(s[1]):\n","            for w in range(s[1]):\n","                ui = u[h,w]\n","                vi = v[h,w]\n","                img[ui, vi, :] = 255.\n","        out.append(img)\n","    return np.float32(np.uint8(out))\n","\n","\n","def highlight_flow_tf(flow, name='flow_to_image'):\n","    \"\"\"Tensorflow ops for highlight flow.\n","    \"\"\"\n","    with tf.variable_scope(name), tf.device('/cpu:0'):\n","        img = tf.py_func(highlight_flow, [flow], tf.float32, stateful=False)\n","        img.set_shape(flow.get_shape().as_list()[0:-1]+[3])\n","        img = img / 127.5 - 1.\n","        return img\n","\n","\n","def image2edge(image):\n","    \"\"\"Convert image to edges.\n","    \"\"\"\n","    out = []\n","    for i in range(image.shape[0]):\n","        img = cv2.Laplacian(image[i, :, :, :], cv2.CV_64F, ksize=3, scale=2)\n","        out.append(img)\n","    return np.float32(np.uint8(out))\n","\n","\"\"\"\n","if __name__ == \"__main__\":\n","    import argparse\n","    parser = argparse.ArgumentParser()\n","    parser.add_argument('--imageA', default='', type=str, help='Image A as background patches to reconstruct image B.')\n","    parser.add_argument('--imageB', default='', type=str, help='Image B is reconstructed with image A.')\n","    parser.add_argument('--imageOut', default='result.png', type=str, help='Image B is reconstructed with image A.')\n","    args = parser.parse_args()\n","    test_contextual_attention(args)\n","\"\"\"\n"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nif __name__ == \"__main__\":\\n    import argparse\\n    parser = argparse.ArgumentParser()\\n    parser.add_argument(\\'--imageA\\', default=\\'\\', type=str, help=\\'Image A as background patches to reconstruct image B.\\')\\n    parser.add_argument(\\'--imageB\\', default=\\'\\', type=str, help=\\'Image B is reconstructed with image A.\\')\\n    parser.add_argument(\\'--imageOut\\', default=\\'result.png\\', type=str, help=\\'Image B is reconstructed with image A.\\')\\n    args = parser.parse_args()\\n    test_contextual_attention(args)\\n'"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"xzoiIA9dVLxh"},"source":["INPAINT_MODEL"]},{"cell_type":"code","metadata":{"id":"jtkcpC0gSv8J","executionInfo":{"status":"ok","timestamp":1605182482651,"user_tz":300,"elapsed":1308,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}}},"source":["\n","import logging\n","import cv2\n","import neuralgym as ng\n","import tensorflow as tf\n","from tensorflow.contrib.framework.python.ops import arg_scope\n","\n","from neuralgym.models import Model\n","from neuralgym.ops.summary_ops import scalar_summary, images_summary\n","from neuralgym.ops.summary_ops import gradients_summary\n","from neuralgym.ops.layers import flatten, resize\n","from neuralgym.ops.gan_ops import gan_hinge_loss\n","from neuralgym.ops.gan_ops import random_interpolates\n","\n","\n","logger = logging.getLogger()\n","\n","\n","class InpaintCAModel(Model):\n","    def __init__(self):\n","        super().__init__('InpaintCAModel')\n","\n","    def build_inpaint_net(self, x, mask, reuse=False,\n","                          training=True, padding='SAME', name='inpaint_net'):\n","        \"\"\"Inpaint network.\n","\n","        Args:\n","            x: incomplete image, [-1, 1]\n","            mask: mask region {0, 1}\n","        Returns:\n","            [-1, 1] as predicted image\n","        \"\"\"\n","        xin = x\n","        offset_flow = None\n","        ones_x = tf.ones_like(x)[:, :, :, 0:1]\n","        x = tf.concat([x, ones_x, ones_x*mask], axis=3)\n","\n","        # two stage network\n","        cnum = 48\n","        with tf.variable_scope(name, reuse=reuse), \\\n","                arg_scope([gen_conv, gen_deconv],\n","                          training=training, padding=padding):\n","            # stage1\n","            x = gen_conv(x, cnum, 5, 1, name='conv1')\n","            x = gen_conv(x, 2*cnum, 3, 2, name='conv2_downsample')\n","            x = gen_conv(x, 2*cnum, 3, 1, name='conv3')\n","            x = gen_conv(x, 4*cnum, 3, 2, name='conv4_downsample')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='conv5')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='conv6')\n","            mask_s = resize_mask_like(mask, x)\n","            x = gen_conv(x, 4*cnum, 3, rate=2, name='conv7_atrous')\n","            x = gen_conv(x, 4*cnum, 3, rate=4, name='conv8_atrous')\n","            x = gen_conv(x, 4*cnum, 3, rate=8, name='conv9_atrous')\n","            x = gen_conv(x, 4*cnum, 3, rate=16, name='conv10_atrous')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='conv11')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='conv12')\n","            x = gen_deconv(x, 2*cnum, name='conv13_upsample')\n","            x = gen_conv(x, 2*cnum, 3, 1, name='conv14')\n","            x = gen_deconv(x, cnum, name='conv15_upsample')\n","            x = gen_conv(x, cnum//2, 3, 1, name='conv16')\n","            x = gen_conv(x, 3, 3, 1, activation=None, name='conv17')\n","            x = tf.nn.tanh(x)\n","            x_stage1 = x\n","\n","            # stage2, paste result as input\n","            x = x*mask + xin[:, :, :, 0:3]*(1.-mask)\n","            x.set_shape(xin[:, :, :, 0:3].get_shape().as_list())\n","            # conv branch\n","            # xnow = tf.concat([x, ones_x, ones_x*mask], axis=3)\n","            xnow = x\n","            x = gen_conv(xnow, cnum, 5, 1, name='xconv1')\n","            x = gen_conv(x, cnum, 3, 2, name='xconv2_downsample')\n","            x = gen_conv(x, 2*cnum, 3, 1, name='xconv3')\n","            x = gen_conv(x, 2*cnum, 3, 2, name='xconv4_downsample')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='xconv5')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='xconv6')\n","            x = gen_conv(x, 4*cnum, 3, rate=2, name='xconv7_atrous')\n","            x = gen_conv(x, 4*cnum, 3, rate=4, name='xconv8_atrous')\n","            x = gen_conv(x, 4*cnum, 3, rate=8, name='xconv9_atrous')\n","            x = gen_conv(x, 4*cnum, 3, rate=16, name='xconv10_atrous')\n","            x_hallu = x\n","            # attention branch\n","            x = gen_conv(xnow, cnum, 5, 1, name='pmconv1')\n","            x = gen_conv(x, cnum, 3, 2, name='pmconv2_downsample')\n","            x = gen_conv(x, 2*cnum, 3, 1, name='pmconv3')\n","            x = gen_conv(x, 4*cnum, 3, 2, name='pmconv4_downsample')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='pmconv5')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='pmconv6',\n","                                activation=tf.nn.relu)\n","            x, offset_flow = contextual_attention(x, x, mask_s, 3, 1, rate=2)\n","            x = gen_conv(x, 4*cnum, 3, 1, name='pmconv9')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='pmconv10')\n","            pm = x\n","            x = tf.concat([x_hallu, pm], axis=3)\n","\n","            x = gen_conv(x, 4*cnum, 3, 1, name='allconv11')\n","            x = gen_conv(x, 4*cnum, 3, 1, name='allconv12')\n","            x = gen_deconv(x, 2*cnum, name='allconv13_upsample')\n","            x = gen_conv(x, 2*cnum, 3, 1, name='allconv14')\n","            x = gen_deconv(x, cnum, name='allconv15_upsample')\n","            x = gen_conv(x, cnum//2, 3, 1, name='allconv16')\n","            x = gen_conv(x, 3, 3, 1, activation=None, name='allconv17')\n","            x = tf.nn.tanh(x)\n","            x_stage2 = x\n","        return x_stage1, x_stage2, offset_flow\n","\n","    def build_sn_patch_gan_discriminator(self, x, reuse=False, training=True):\n","        with tf.variable_scope('sn_patch_gan', reuse=reuse):\n","            cnum = 64\n","            x = dis_conv(x, cnum, name='conv1', training=training)\n","            x = dis_conv(x, cnum*2, name='conv2', training=training)\n","            x = dis_conv(x, cnum*4, name='conv3', training=training)\n","            x = dis_conv(x, cnum*4, name='conv4', training=training)\n","            x = dis_conv(x, cnum*4, name='conv5', training=training)\n","            x = dis_conv(x, cnum*4, name='conv6', training=training)\n","            x = flatten(x, name='flatten')\n","            return x\n","\n","    def build_gan_discriminator(\n","            self, batch, reuse=False, training=True):\n","        with tf.variable_scope('discriminator', reuse=reuse):\n","            d = self.build_sn_patch_gan_discriminator(\n","                batch, reuse=reuse, training=training)\n","            return d\n","\n","    def build_graph_with_losses(\n","            self, FLAGS, batch_data, training=True, summary=False,\n","            reuse=False):\n","        if FLAGS.guided:\n","            batch_data, edge = batch_data\n","            edge = edge[:, :, :, 0:1] / 255.\n","            edge = tf.cast(edge > FLAGS.edge_threshold, tf.float32)\n","        batch_pos = batch_data / 127.5 - 1.\n","        # generate mask, 1 represents masked point\n","        bbox = random_bbox(FLAGS)\n","        regular_mask = bbox2mask(FLAGS, bbox, name='mask_c')\n","        irregular_mask = brush_stroke_mask(FLAGS, name='mask_c')\n","        mask = tf.cast(\n","            tf.logical_or(\n","                tf.cast(irregular_mask, tf.bool),\n","                tf.cast(regular_mask, tf.bool),\n","            ),\n","            tf.float32\n","        )\n","\n","        batch_incomplete = batch_pos*(1.-mask)\n","        if FLAGS.guided:\n","            edge = edge * mask\n","            xin = tf.concat([batch_incomplete, edge], axis=3)\n","        else:\n","            xin = batch_incomplete\n","        x1, x2, offset_flow = self.build_inpaint_net(\n","            xin, mask, reuse=reuse, training=training,\n","            padding=FLAGS.padding)\n","        batch_predicted = x2\n","        losses = {}\n","        # apply mask and complete image\n","        batch_complete = batch_predicted*mask + batch_incomplete*(1.-mask)\n","        # local patches\n","        losses['ae_loss'] = FLAGS.l1_loss_alpha * tf.reduce_mean(tf.abs(batch_pos - x1))\n","        losses['ae_loss'] += FLAGS.l1_loss_alpha * tf.reduce_mean(tf.abs(batch_pos - x2))\n","        if summary:\n","            scalar_summary('losses/ae_loss', losses['ae_loss'])\n","            if FLAGS.guided:\n","                viz_img = [\n","                    batch_pos,\n","                    batch_incomplete + edge,\n","                    batch_complete]\n","            else:\n","                viz_img = [batch_pos, batch_incomplete, batch_complete]\n","            if offset_flow is not None:\n","                viz_img.append(\n","                    resize(offset_flow, scale=4,\n","                           func=tf.image.resize_bilinear))\n","            images_summary(\n","                tf.concat(viz_img, axis=2),\n","                'raw_incomplete_predicted_complete', FLAGS.viz_max_out)\n","\n","        # gan\n","        batch_pos_neg = tf.concat([batch_pos, batch_complete], axis=0)\n","        if FLAGS.gan_with_mask:\n","            batch_pos_neg = tf.concat([batch_pos_neg, tf.tile(mask, [FLAGS.batch_size*2, 1, 1, 1])], axis=3)\n","        if FLAGS.guided:\n","            # conditional GANs\n","            batch_pos_neg = tf.concat([batch_pos_neg, tf.tile(edge, [2, 1, 1, 1])], axis=3)\n","        # wgan with gradient penalty\n","        if FLAGS.gan == 'sngan':\n","            pos_neg = self.build_gan_discriminator(batch_pos_neg, training=training, reuse=reuse)\n","            pos, neg = tf.split(pos_neg, 2)\n","            g_loss, d_loss = gan_hinge_loss(pos, neg)\n","            losses['g_loss'] = g_loss\n","            losses['d_loss'] = d_loss\n","        else:\n","            raise NotImplementedError('{} not implemented.'.format(FLAGS.gan))\n","        if summary:\n","            # summary the magnitude of gradients from different losses w.r.t. predicted image\n","            gradients_summary(losses['g_loss'], batch_predicted, name='g_loss')\n","            gradients_summary(losses['g_loss'], x2, name='g_loss_to_x2')\n","            # gradients_summary(losses['ae_loss'], x1, name='ae_loss_to_x1')\n","            gradients_summary(losses['ae_loss'], x2, name='ae_loss_to_x2')\n","        losses['g_loss'] = FLAGS.gan_loss_alpha * losses['g_loss']\n","        if FLAGS.ae_loss:\n","            losses['g_loss'] += losses['ae_loss']\n","        g_vars = tf.get_collection(\n","            tf.GraphKeys.TRAINABLE_VARIABLES, 'inpaint_net')\n","        d_vars = tf.get_collection(\n","            tf.GraphKeys.TRAINABLE_VARIABLES, 'discriminator')\n","        return g_vars, d_vars, losses\n","\n","    def build_infer_graph(self, FLAGS, batch_data, bbox=None, name='val'):\n","        \"\"\"\n","        \"\"\"\n","        if FLAGS.guided:\n","            batch_data, edge = batch_data\n","            edge = edge[:, :, :, 0:1] / 255.\n","            edge = tf.cast(edge > FLAGS.edge_threshold, tf.float32)\n","        regular_mask = bbox2mask(FLAGS, bbox, name='mask_c')\n","        irregular_mask = brush_stroke_mask(FLAGS, name='mask_c')\n","        mask = tf.cast(\n","            tf.logical_or(\n","                tf.cast(irregular_mask, tf.bool),\n","                tf.cast(regular_mask, tf.bool),\n","            ),\n","            tf.float32\n","        )\n","\n","        batch_pos = batch_data / 127.5 - 1.\n","        batch_incomplete = batch_pos*(1.-mask)\n","        if FLAGS.guided:\n","            edge = edge * mask\n","            xin = tf.concat([batch_incomplete, edge], axis=3)\n","        else:\n","            xin = batch_incomplete\n","        # inpaint\n","        x1, x2, offset_flow = self.build_inpaint_net(\n","            xin, mask, reuse=True,\n","            training=False, padding=FLAGS.padding)\n","        batch_predicted = x2\n","        # apply mask and reconstruct\n","        batch_complete = batch_predicted*mask + batch_incomplete*(1.-mask)\n","        # global image visualization\n","        if FLAGS.guided:\n","            viz_img = [\n","                batch_pos,\n","                batch_incomplete + edge,\n","                batch_complete]\n","        else:\n","            viz_img = [batch_pos, batch_incomplete, batch_complete]\n","        if offset_flow is not None:\n","            viz_img.append(\n","                resize(offset_flow, scale=4,\n","                       func=tf.image.resize_bilinear))\n","        images_summary(\n","            tf.concat(viz_img, axis=2),\n","            name+'_raw_incomplete_complete', FLAGS.viz_max_out)\n","        return batch_complete\n","\n","    def build_static_infer_graph(self, FLAGS, batch_data, name):\n","        \"\"\"\n","        \"\"\"\n","        # generate mask, 1 represents masked point\n","        bbox = (tf.constant(FLAGS.height//2), tf.constant(FLAGS.width//2),\n","                tf.constant(FLAGS.height), tf.constant(FLAGS.width))\n","        return self.build_infer_graph(FLAGS, batch_data, bbox, name)\n","\n","\n","    def build_server_graph(self, FLAGS, batch_data, reuse=False, is_training=False):\n","        \"\"\"\n","        \"\"\"\n","        # generate mask, 1 represents masked point\n","        if FLAGS.guided:\n","            batch_raw, edge, masks_raw = tf.split(batch_data, 3, axis=2)\n","            edge = edge[:, :, :, 0:1] / 255.\n","            edge = tf.cast(edge > FLAGS.edge_threshold, tf.float32)\n","        else:\n","            batch_raw, masks_raw = tf.split(batch_data, 2, axis=2)\n","        masks = tf.cast(masks_raw[0:1, :, :, 0:1] > 127.5, tf.float32)\n","\n","        batch_pos = batch_raw / 127.5 - 1.\n","        batch_incomplete = batch_pos * (1. - masks)\n","        if FLAGS.guided:\n","            edge = edge * masks[:, :, :, 0:1]\n","            xin = tf.concat([batch_incomplete, edge], axis=3)\n","        else:\n","            xin = batch_incomplete\n","        # inpaint\n","        x1, x2, flow = self.build_inpaint_net(\n","            xin, masks, reuse=reuse, training=is_training)\n","        batch_predict = x2\n","        # apply mask and reconstruct\n","        batch_complete = batch_predict*masks + batch_incomplete*(1-masks)\n","        return batch_complete"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"STOBDPxCSpDe","executionInfo":{"status":"error","timestamp":1605182951076,"user_tz":300,"elapsed":999,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"1891b350-ff55-4520-c865-c0152cdc90c4","colab":{"base_uri":"https://localhost:8080/","height":948}},"source":["import os\n","import glob\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","import neuralgym as ng\n","\n","\n","def multigpu_graph_def(model, FLAGS, data, gpu_id=0, loss_type='g'):\n","    with tf.device('/cpu:0'):\n","        images = data.data_pipeline(FLAGS.batch_size)\n","    if gpu_id == 0 and loss_type == 'g':\n","        _, _, losses = model.build_graph_with_losses(\n","            FLAGS, images, FLAGS, summary=True, reuse=True)\n","    else:\n","        _, _, losses = model.build_graph_with_losses(\n","            FLAGS, images, FLAGS, reuse=True)\n","    if loss_type == 'g':\n","        return losses['g_loss']\n","    elif loss_type == 'd':\n","        return losses['d_loss']\n","    else:\n","        raise ValueError('loss type is not supported.')\n","\n","\n","if __name__ == \"__main__\":\n","    # training data\n","    FLAGS = ng.Config('inpaint.yml')\n","    img_shapes = FLAGS.img_shapes\n","    with open(FLAGS.data_flist[FLAGS.dataset][0]) as f:\n","        fnames = f.read().splitlines()\n","    if FLAGS.guided:\n","        fnames = [(fname, fname[:-4] + '_edge.jpg') for fname in fnames]\n","        img_shapes = [img_shapes, img_shapes]\n","    data = ng.data.DataFromFNames(\n","        fnames, img_shapes, random_crop=FLAGS.random_crop,\n","        nthreads=FLAGS.num_cpus_per_job)\n","    images = data.data_pipeline(FLAGS.batch_size)\n","    # main model\n","    model = InpaintCAModel()\n","    g_vars, d_vars, losses = model.build_graph_with_losses(FLAGS, images)\n","    # validation images\n","    if FLAGS.val:\n","        with open(FLAGS.data_flist[FLAGS.dataset][1]) as f:\n","            val_fnames = f.read().splitlines()\n","        if FLAGS.guided:\n","            val_fnames = [\n","                (fname, fname[:-4] + '_edge.jpg') for fname in val_fnames]\n","        # progress monitor by visualizing static images\n","        for i in range(FLAGS.static_view_size):\n","            static_fnames = val_fnames[i:i+1]\n","            static_images = ng.data.DataFromFNames(\n","                static_fnames, img_shapes, nthreads=1,\n","                random_crop=FLAGS.random_crop).data_pipeline(1)\n","            static_inpainted_images = model.build_static_infer_graph(\n","                FLAGS, static_images, name='static_view/%d' % i)\n","    # training settings\n","    lr = tf.get_variable(\n","        'lr', shape=[], trainable=False,\n","        initializer=tf.constant_initializer(1e-4))\n","    d_optimizer = tf.train.AdamOptimizer(lr, beta1=0.5, beta2=0.999)\n","    g_optimizer = d_optimizer\n","    # train discriminator with secondary trainer, should initialize before\n","    # primary trainer.\n","    # discriminator_training_callback = ng.callbacks.SecondaryTrainer(\n","    discriminator_training_callback = ng.callbacks.SecondaryMultiGPUTrainer(\n","        num_gpus=FLAGS.num_gpus_per_job,\n","        pstep=1,\n","        optimizer=d_optimizer,\n","        var_list=d_vars,\n","        max_iters=1,\n","        grads_summary=False,\n","        graph_def=multigpu_graph_def,\n","        graph_def_kwargs={\n","            'model': model, 'FLAGS': FLAGS, 'data': data, 'loss_type': 'd'},\n","    )\n","    # train generator with primary trainer\n","    # trainer = ng.train.Trainer(\n","    trainer = ng.train.MultiGPUTrainer(\n","        num_gpus=FLAGS.num_gpus_per_job,\n","        optimizer=g_optimizer,\n","        var_list=g_vars,\n","        max_iters=FLAGS.max_iters,\n","        graph_def=multigpu_graph_def,\n","        grads_summary=False,\n","        gradient_processor=None,\n","        graph_def_kwargs={\n","            'model': model, 'FLAGS': FLAGS, 'data': data, 'loss_type': 'g'},\n","        spe=FLAGS.train_spe,\n","        log_dir=FLAGS.log_dir,\n","    )\n","    # add all callbacks\n","    trainer.add_callbacks([\n","        discriminator_training_callback,\n","        ng.callbacks.WeightsViewer(),\n","        ng.callbacks.ModelRestorer(trainer.context['saver'], dump_prefix=FLAGS.model_restore+'/snap', optimistic=True),\n","        ng.callbacks.ModelSaver(FLAGS.train_spe, trainer.context['saver'], FLAGS.log_dir+'/snap'),\n","        ng.callbacks.SummaryWriter((FLAGS.val_psteps//1), trainer.context['summary_writer'], tf.summary.merge_all()),\n","    ])\n","    # launch training\n","    trainer.train()"],"execution_count":21,"outputs":[{"output_type":"stream","text":["---------------------------------- APP CONFIG ----------------------------------\n","num_gpus_per_job: 1\n","num_cpus_per_job: 4\n","num_hosts_per_job: 1\n","memory_per_job: 32\n","gpu_type: nvidia-tesla-p100\n","name: places2_gated_conv_v100\n","model_restore: \n","dataset: celebahq\n","random_crop: False\n","val: False\n","log_dir: logs/full_model_celeba_hq_256\n","gan: sngan\n","gan_loss_alpha: 1\n","gan_with_mask: True\n","discounted_mask: True\n","random_seed: False\n","padding: SAME\n","train_spe: 4000\n","max_iters: 100000000\n","viz_max_out: 10\n","val_psteps: 2000\n","data_flist: \n","  celebahq: ['data/celeba_hq/train_shuffled.flist', 'data/celeba_hq/validation_static_view.flist']\n","  celeba: ['data/celeba/train_shuffled.flist', 'data/celeba/validation_static_view.flist']\n","  places2: ['data/places2/train_shuffled.flist', 'data/places2/validation_static_view.flist']\n","  imagenet: ['data/imagenet/train_shuffled.flist', 'data/imagenet/validation_static_view.flist']\n","static_view_size: 30\n","img_shapes: [256, 256, 3]\n","height: 128\n","width: 128\n","max_delta_height: 32\n","max_delta_width: 32\n","batch_size: 16\n","vertical_margin: 0\n","horizontal_margin: 0\n","ae_loss: True\n","l1_loss: True\n","l1_loss_alpha: 1.0\n","guided: False\n","edge_threshold: 0.6\n","--------------------------------------------------------------------------------\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-4cef1d2a62cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mFLAGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mng\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inpaint.yml'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mimg_shapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_shapes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_flist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mfnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguided\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/celeba_hq/train_shuffled.flist'"]}]}]}