{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Partial Convolution.ipynb","provenance":[{"file_id":"1IzPbxNFQmmSYk9s14L4YjBfUgACn9mW2","timestamp":1596153089372},{"file_id":"1G7NKeneJNyRtcRxLVbbF9jYtRyuTOa-R","timestamp":1592749700622},{"file_id":"https://github.com/satyajitghana/TSAI-DeepVision-EVA4.0/blob/master/Utils/Colab_25GBRAM_GPU.ipynb","timestamp":1592043804148}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"YlfaeL-0Itkb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607609642875,"user_tz":300,"elapsed":988,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"1156bbf6-cccb-42c3-c0e7-7a96c62903f8"},"source":["!nvidia-smi\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Thu Dec 10 14:14:02 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 455.45.01    Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   39C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n","|                               |                      |                 ERR! |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Qex2EbDjJIy6"},"source":["DESCARGAR DATASET\n","\n"]},{"cell_type":"code","metadata":{"id":"ZV6_E6IoJvzD"},"source":["#!wget http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDD8z2nmJPel"},"source":["CONECTAR CON GOOGLE DRIVE"]},{"cell_type":"code","metadata":{"id":"T8Sds2_sKsIg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608210144917,"user_tz":300,"elapsed":361578,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"dd7c97e3-2b88-484c-d75f-3baae96ba83e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OeIcT6GkIin4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608210149144,"user_tz":300,"elapsed":2616,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"b4158044-15e0-4be5-ff57-9973572c61d5"},"source":["import os\n","os.chdir(\"/content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data\")\n","# DIV2K_train_HR.zip\n","!ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["masks  test  train\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mHh1N3a4FKv5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608210201866,"user_tz":300,"elapsed":1489,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"414141a9-da2b-40b5-9cf0-475d16e3c259"},"source":["os.chdir(\"/content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train\")\n","print(len(os.listdir()))\n","os.chdir(\"/content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/test\")\n","len(os.listdir())"],"execution_count":5,"outputs":[{"output_type":"stream","text":["286\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["514"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"Ud-c77eBhg1m","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1608210212589,"user_tz":300,"elapsed":4759,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"7b9ccfc8-3d21-44c8-e9f5-d13d84323723"},"source":["!pip install keras-tqdm\n","%tensorflow_version 1.x"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Collecting keras-tqdm\n","  Downloading https://files.pythonhosted.org/packages/16/5c/ac63c65b79a895b8994474de2ad4d5b66ac0796b8903d60cfea3f8308d5c/keras_tqdm-2.0.1-py2.py3-none-any.whl\n","Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (2.4.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from keras-tqdm) (4.41.1)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.18.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-tqdm) (2.10.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->Keras->keras-tqdm) (1.15.0)\n","Installing collected packages: keras-tqdm\n","Successfully installed keras-tqdm-2.0.1\n","TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uC8SD0kVBjdF"},"source":["CONVOLUCION PARCIAL"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-htpWaojBSyB","executionInfo":{"status":"ok","timestamp":1608210220500,"user_tz":300,"elapsed":6393,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"78d2834c-5457-42d6-d989-f41f155617a4"},"source":["from keras.utils import conv_utils\n","from keras import backend as K\n","from keras.engine import InputSpec\n","from keras.layers import Conv2D\n","\n","\n","class PConv2D(Conv2D):\n","    def __init__(self, *args, n_channels=3, mono=False, **kwargs):\n","        super().__init__(*args, **kwargs)\n","        self.input_spec = [InputSpec(ndim=4), InputSpec(ndim=4)]\n","\n","    def build(self, input_shape):        \n","        \"\"\"Adapted from original _Conv() layer of Keras        \n","        param input_shape: list of dimensions for [img, mask]\n","        \"\"\"\n","        \n","        if self.data_format == 'channels_first':\n","            channel_axis = 1\n","        else:\n","            channel_axis = -1\n","            \n","        if input_shape[0][channel_axis] is None:\n","            raise ValueError('The channel dimension of the inputs should be defined. Found `None`.')\n","            \n","        self.input_dim = input_shape[0][channel_axis]\n","        \n","        # Image kernel\n","        kernel_shape = self.kernel_size + (self.input_dim, self.filters)\n","        self.kernel = self.add_weight(shape=kernel_shape,\n","                                      initializer=self.kernel_initializer,\n","                                      name='img_kernel',\n","                                      regularizer=self.kernel_regularizer,\n","                                      constraint=self.kernel_constraint)\n","        # Mask kernel\n","        self.kernel_mask = K.ones(shape=self.kernel_size + (self.input_dim, self.filters))\n","\n","        # Calculate padding size to achieve zero-padding\n","        self.pconv_padding = (\n","            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n","            (int((self.kernel_size[0]-1)/2), int((self.kernel_size[0]-1)/2)), \n","        )\n","\n","        # Window size - used for normalization\n","        self.window_size = self.kernel_size[0] * self.kernel_size[1]\n","        \n","        if self.use_bias:\n","            self.bias = self.add_weight(shape=(self.filters,),\n","                                        initializer=self.bias_initializer,\n","                                        name='bias',\n","                                        regularizer=self.bias_regularizer,\n","                                        constraint=self.bias_constraint)\n","        else:\n","            self.bias = None\n","        self.built = True\n","\n","    def call(self, inputs, mask=None):\n","        '''\n","        We will be using the Keras conv2d method, and essentially we have\n","        to do here is multiply the mask with the input X, before we apply the\n","        convolutions. For the mask itself, we apply convolutions with all weights\n","        set to 1.\n","        Subsequently, we clip mask values to between 0 and 1\n","        ''' \n","\n","        # Both image and mask must be supplied\n","        if type(inputs) is not list or len(inputs) != 2:\n","            raise Exception('PartialConvolution2D must be called on a list of two tensors [img, mask]. Instead got: ' + str(inputs))\n","\n","        # Padding done explicitly so that padding becomes part of the masked partial convolution\n","        images = K.spatial_2d_padding(inputs[0], self.pconv_padding, self.data_format)\n","        masks = K.spatial_2d_padding(inputs[1], self.pconv_padding, self.data_format)\n","\n","        # Apply convolutions to mask\n","        mask_output = K.conv2d(\n","            masks, self.kernel_mask, \n","            strides=self.strides,\n","            padding='valid',\n","            data_format=self.data_format,\n","            dilation_rate=self.dilation_rate\n","        )\n","\n","        # Apply convolutions to image\n","        img_output = K.conv2d(\n","            (images*masks), self.kernel, \n","            strides=self.strides,\n","            padding='valid',\n","            data_format=self.data_format,\n","            dilation_rate=self.dilation_rate\n","        )        \n","\n","        # Calculate the mask ratio on each pixel in the output mask\n","        mask_ratio = self.window_size / (mask_output + 1e-8)\n","\n","        # Clip output to be between 0 and 1\n","        mask_output = K.clip(mask_output, 0, 1)\n","\n","        # Remove ratio values where there are holes\n","        mask_ratio = mask_ratio * mask_output\n","\n","        # Normalize iamge output\n","        img_output = img_output * mask_ratio\n","\n","        # Apply bias only to the image (if chosen to do so)\n","        if self.use_bias:\n","            img_output = K.bias_add(\n","                img_output,\n","                self.bias,\n","                data_format=self.data_format)\n","        \n","        # Apply activations on the image\n","        if self.activation is not None:\n","            img_output = self.activation(img_output)\n","            \n","        return [img_output, mask_output]\n","    \n","    def compute_output_shape(self, input_shape):\n","        if self.data_format == 'channels_last':\n","            space = input_shape[0][1:-1]\n","            new_space = []\n","            for i in range(len(space)):\n","                new_dim = conv_utils.conv_output_length(\n","                    space[i],\n","                    self.kernel_size[i],\n","                    padding='same',\n","                    stride=self.strides[i],\n","                    dilation=self.dilation_rate[i])\n","                new_space.append(new_dim)\n","            new_shape = (input_shape[0][0],) + tuple(new_space) + (self.filters,)\n","            return [new_shape, new_shape]\n","        if self.data_format == 'channels_first':\n","            space = input_shape[2:]\n","            new_space = []\n","            for i in range(len(space)):\n","                new_dim = conv_utils.conv_output_length(\n","                    space[i],\n","                    self.kernel_size[i],\n","                    padding='same',\n","                    stride=self.strides[i],\n","                    dilation=self.dilation_rate[i])\n","                new_space.append(new_dim)\n","            new_shape = (input_shape[0], self.filters) + tuple(new_space)\n","            return [new_shape, new_shape]"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"z2p2Vp9mBq_c"},"source":["ARQUITECTURA Unet"]},{"cell_type":"code","metadata":{"id":"licbiY2-BtkF","executionInfo":{"status":"ok","timestamp":1608210224996,"user_tz":300,"elapsed":1553,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}}},"source":["import os\n","import sys\n","import numpy as np\n","from datetime import datetime\n","\n","import tensorflow as tf\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.optimizers import Adam\n","from keras.layers import Input, Conv2D, UpSampling2D, Dropout, LeakyReLU, BatchNormalization, Activation, Lambda\n","from keras.layers.merge import Concatenate\n","from keras.applications import VGG16\n","from keras import backend as K\n","from keras.utils.multi_gpu_utils import multi_gpu_model\n","\n","\n","\n","class PConvUnet(object):\n","\n","    def __init__(self, img_rows=512, img_cols=512, vgg_weights=\"imagenet\", inference_only=False, net_name='default', gpus=1, vgg_device=None):\n","        \"\"\"Create the PConvUnet. If variable image size, set img_rows and img_cols to None\n","        \n","        Args:\n","            img_rows (int): image height.\n","            img_cols (int): image width.\n","            vgg_weights (str): which weights to pass to the vgg network.\n","            inference_only (bool): initialize BN layers for inference.\n","            net_name (str): Name of this network (used in logging).\n","            gpus (int): How many GPUs to use for training.\n","            vgg_device (str): In case of training with multiple GPUs, specify which device to run VGG inference on.\n","                e.g. if training on 8 GPUs, vgg inference could be off-loaded exclusively to one GPU, instead of\n","                running on one of the GPUs which is also training the UNet.\n","        \"\"\"\n","        \n","        # Settings\n","        self.img_rows = img_rows\n","        self.img_cols = img_cols\n","        self.img_overlap = 30\n","        self.inference_only = inference_only\n","        self.net_name = net_name\n","        self.gpus = gpus\n","        self.vgg_device = vgg_device\n","\n","        # Scaling for VGG input\n","        self.mean = [0.485, 0.456, 0.406]\n","        self.std = [0.229, 0.224, 0.225]\n","\n","        # Assertions\n","        assert self.img_rows >= 256, 'Height must be >256 pixels'\n","        assert self.img_cols >= 256, 'Width must be >256 pixels'\n","\n","        # Set current epoch\n","        self.current_epoch = 0\n","        \n","        # VGG layers to extract features from (first maxpooling layers, see pp. 7 of paper)\n","        self.vgg_layers = [3, 6, 10]\n","\n","        # Instantiate the vgg network\n","        if self.vgg_device:\n","            with tf.device(self.vgg_device):\n","                self.vgg = self.build_vgg(vgg_weights)\n","        else:\n","            self.vgg = self.build_vgg(vgg_weights)\n","        \n","        # Create UNet-like model\n","        if self.gpus <= 1:\n","            self.model, inputs_mask = self.build_pconv_unet()\n","            self.compile_pconv_unet(self.model, inputs_mask)            \n","        else:\n","            with tf.device(\"/cpu:0\"):\n","                self.model, inputs_mask = self.build_pconv_unet()\n","            self.model = multi_gpu_model(self.model, gpus=self.gpus)\n","            self.compile_pconv_unet(self.model, inputs_mask)\n","        \n","    def build_vgg(self, weights=\"imagenet\"):\n","        \"\"\"\n","        Load pre-trained VGG16 from keras applications\n","        Extract features to be used in loss function from last conv layer, see architecture at:\n","        https://github.com/keras-team/keras/blob/master/keras/applications/vgg16.py\n","        \"\"\"        \n","            \n","        # Input image to extract features from\n","        img = Input(shape=(self.img_rows, self.img_cols, 3))\n","\n","        # Mean center and rescale by variance as in PyTorch\n","        processed = Lambda(lambda x: (x-self.mean) / self.std)(img)\n","        \n","        # If inference only, just return empty model        \n","        if self.inference_only:\n","            model = Model(inputs=img, outputs=[img for _ in range(len(self.vgg_layers))])\n","            model.trainable = False\n","            model.compile(loss='mse', optimizer='adam')\n","            return model\n","                \n","        # Get the vgg network from Keras applications\n","        if weights in ['imagenet', None]:\n","            vgg = VGG16(weights=weights, include_top=False)\n","        else:\n","            vgg = VGG16(weights=None, include_top=False)\n","            vgg.load_weights(weights, by_name=True)\n","\n","        # Output the first three pooling layers\n","        vgg.outputs = [vgg.layers[i].output for i in self.vgg_layers]        \n","        \n","        # Create model and compile\n","        model = Model(inputs=img, outputs=vgg(processed))\n","        model.trainable = False\n","        model.compile(loss='mse', optimizer='adam')\n","\n","        return model\n","        \n","    def build_pconv_unet(self, train_bn=True):      \n","\n","        # INPUTS\n","        inputs_img = Input((self.img_rows, self.img_cols, 3), name='inputs_img')\n","        inputs_mask = Input((self.img_rows, self.img_cols, 3), name='inputs_mask')\n","        \n","        # ENCODER\n","        def encoder_layer(img_in, mask_in, filters, kernel_size, bn=True):\n","            conv, mask = PConv2D(filters, kernel_size, strides=2, padding='same')([img_in, mask_in])\n","            if bn:\n","                conv = BatchNormalization(name='EncBN'+str(encoder_layer.counter))(conv, training=train_bn)\n","            conv = Activation('relu')(conv)\n","            encoder_layer.counter += 1\n","            return conv, mask\n","        encoder_layer.counter = 0\n","        \n","        e_conv1, e_mask1 = encoder_layer(inputs_img, inputs_mask, 64, 7, bn=False)\n","        e_conv2, e_mask2 = encoder_layer(e_conv1, e_mask1, 128, 5)\n","        e_conv3, e_mask3 = encoder_layer(e_conv2, e_mask2, 256, 5)\n","        e_conv4, e_mask4 = encoder_layer(e_conv3, e_mask3, 512, 3)\n","        e_conv5, e_mask5 = encoder_layer(e_conv4, e_mask4, 512, 3)\n","        e_conv6, e_mask6 = encoder_layer(e_conv5, e_mask5, 512, 3)\n","        e_conv7, e_mask7 = encoder_layer(e_conv6, e_mask6, 512, 3)\n","        e_conv8, e_mask8 = encoder_layer(e_conv7, e_mask7, 512, 3)\n","        \n","        # DECODER\n","        def decoder_layer(img_in, mask_in, e_conv, e_mask, filters, kernel_size, bn=True):\n","            up_img = UpSampling2D(size=(2,2))(img_in)\n","            up_mask = UpSampling2D(size=(2,2))(mask_in)\n","            concat_img = Concatenate(axis=3)([e_conv,up_img])\n","            concat_mask = Concatenate(axis=3)([e_mask,up_mask])\n","            conv, mask = PConv2D(filters, kernel_size, padding='same')([concat_img, concat_mask])\n","            if bn:\n","                conv = BatchNormalization()(conv)\n","            conv = LeakyReLU(alpha=0.2)(conv)\n","            return conv, mask\n","            \n","        d_conv9, d_mask9 = decoder_layer(e_conv8, e_mask8, e_conv7, e_mask7, 512, 3)\n","        d_conv10, d_mask10 = decoder_layer(d_conv9, d_mask9, e_conv6, e_mask6, 512, 3)\n","        d_conv11, d_mask11 = decoder_layer(d_conv10, d_mask10, e_conv5, e_mask5, 512, 3)\n","        d_conv12, d_mask12 = decoder_layer(d_conv11, d_mask11, e_conv4, e_mask4, 512, 3)\n","        d_conv13, d_mask13 = decoder_layer(d_conv12, d_mask12, e_conv3, e_mask3, 256, 3)\n","        d_conv14, d_mask14 = decoder_layer(d_conv13, d_mask13, e_conv2, e_mask2, 128, 3)\n","        d_conv15, d_mask15 = decoder_layer(d_conv14, d_mask14, e_conv1, e_mask1, 64, 3)\n","        d_conv16, d_mask16 = decoder_layer(d_conv15, d_mask15, inputs_img, inputs_mask, 3, 3, bn=False)\n","        outputs = Conv2D(3, 1, activation = 'sigmoid', name='outputs_img')(d_conv16)\n","        \n","        # Setup the model inputs / outputs\n","        model = Model(inputs=[inputs_img, inputs_mask], outputs=outputs)\n","\n","        return model, inputs_mask    \n","\n","    def compile_pconv_unet(self, model, inputs_mask, lr=0.0002):\n","        model.compile(\n","            optimizer = Adam(lr=lr),\n","            loss=self.loss_total(inputs_mask),\n","            metrics=[self.PSNR]\n","        )\n","\n","    def loss_total(self, mask):\n","        \"\"\"\n","        Creates a loss function which sums all the loss components \n","        and multiplies by their weights. See paper eq. 7.\n","        \"\"\"\n","        def loss(y_true, y_pred):\n","\n","            # Compute predicted image with non-hole pixels set to ground truth\n","            y_comp = mask * y_true + (1-mask) * y_pred\n","\n","            # Compute the vgg features. \n","            if self.vgg_device:\n","                with tf.device(self.vgg_device):\n","                    vgg_out = self.vgg(y_pred)\n","                    vgg_gt = self.vgg(y_true)\n","                    vgg_comp = self.vgg(y_comp)\n","            else:\n","                vgg_out = self.vgg(y_pred)\n","                vgg_gt = self.vgg(y_true)\n","                vgg_comp = self.vgg(y_comp)\n","            \n","            # Compute loss components\n","            l1 = self.loss_valid(mask, y_true, y_pred)\n","            l2 = self.loss_hole(mask, y_true, y_pred)\n","            l3 = self.loss_perceptual(vgg_out, vgg_gt, vgg_comp)\n","            l4 = self.loss_style(vgg_out, vgg_gt)\n","            l5 = self.loss_style(vgg_comp, vgg_gt)\n","            l6 = self.loss_tv(mask, y_comp)\n","\n","            # Return loss function\n","            return l1 + 6*l2 + 0.05*l3 + 120*(l4+l5) + 0.1*l6\n","\n","        return loss\n","\n","    def loss_hole(self, mask, y_true, y_pred):\n","        \"\"\"Pixel L1 loss within the hole / mask\"\"\"\n","        return self.l1((1-mask) * y_true, (1-mask) * y_pred)\n","    \n","    def loss_valid(self, mask, y_true, y_pred):\n","        \"\"\"Pixel L1 loss outside the hole / mask\"\"\"\n","        return self.l1(mask * y_true, mask * y_pred)\n","    \n","    def loss_perceptual(self, vgg_out, vgg_gt, vgg_comp): \n","        \"\"\"Perceptual loss based on VGG16, see. eq. 3 in paper\"\"\"       \n","        loss = 0\n","        for o, c, g in zip(vgg_out, vgg_comp, vgg_gt):\n","            loss += self.l1(o, g) + self.l1(c, g)\n","        return loss\n","        \n","    def loss_style(self, output, vgg_gt):\n","        \"\"\"Style loss based on output/computation, used for both eq. 4 & 5 in paper\"\"\"\n","        loss = 0\n","        for o, g in zip(output, vgg_gt):\n","            loss += self.l1(self.gram_matrix(o), self.gram_matrix(g))\n","        return loss\n","    \n","    def loss_tv(self, mask, y_comp):\n","        \"\"\"Total variation loss, used for smoothing the hole region, see. eq. 6\"\"\"\n","\n","        # Create dilated hole region using a 3x3 kernel of all 1s.\n","        kernel = K.ones(shape=(3, 3, mask.shape[3], mask.shape[3]))\n","        dilated_mask = K.conv2d(1-mask, kernel, data_format='channels_last', padding='same')\n","\n","        # Cast values to be [0., 1.], and compute dilated hole region of y_comp\n","        dilated_mask = K.cast(K.greater(dilated_mask, 0), 'float32')\n","        P = dilated_mask * y_comp\n","\n","        # Calculate total variation loss\n","        a = self.l1(P[:,1:,:,:], P[:,:-1,:,:])\n","        b = self.l1(P[:,:,1:,:], P[:,:,:-1,:])        \n","        return a+b\n","\n","    def fit_generator(self, generator, *args, **kwargs):\n","        \"\"\"Fit the U-Net to a (images, targets) generator\n","        Args:\n","            generator (generator): generator supplying input image & mask, as well as targets.\n","            *args: arguments to be passed to fit_generator\n","            **kwargs: keyword arguments to be passed to fit_generator\n","        \"\"\"\n","        self.model.fit_generator(\n","            generator,\n","            *args, **kwargs\n","        )\n","        \n","    def summary(self):\n","        \"\"\"Get summary of the UNet model\"\"\"\n","        print(self.model.summary())\n","\n","    def load(self, filepath, train_bn=True, lr=0.0002):\n","\n","        # Create UNet-like model\n","        self.model, inputs_mask = self.build_pconv_unet(train_bn)\n","        self.compile_pconv_unet(self.model, inputs_mask, lr) \n","\n","        # Load weights into model\n","        epoch = int(os.path.basename(filepath).split('.')[1].split('-')[0])\n","        assert epoch > 0, \"Could not parse weight file. Should include the epoch\"\n","        self.current_epoch = epoch\n","        self.model.load_weights(filepath)        \n","\n","    @staticmethod\n","    def PSNR(y_true, y_pred):\n","        \"\"\"\n","        PSNR is Peek Signal to Noise Ratio, see https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio\n","        The equation is:\n","        PSNR = 20 * log10(MAX_I) - 10 * log10(MSE)\n","        \n","        Our input is scaled with be within the range -2.11 to 2.64 (imagenet value scaling). We use the difference between these\n","        two values (4.75) as MAX_I        \n","        \"\"\"        \n","        #return 20 * K.log(4.75) / K.log(10.0) - 10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0) \n","        return - 10.0 * K.log(K.mean(K.square(y_pred - y_true))) / K.log(10.0) \n","\n","    @staticmethod\n","    def current_timestamp():\n","        return datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n","    \n","    @staticmethod\n","    def l1(y_true, y_pred):\n","        \"\"\"Calculate the L1 loss used in all loss calculations\"\"\"\n","        if K.ndim(y_true) == 4:\n","            return K.mean(K.abs(y_pred - y_true), axis=[1,2,3])\n","        elif K.ndim(y_true) == 3:\n","            return K.mean(K.abs(y_pred - y_true), axis=[1,2])\n","        else:\n","            raise NotImplementedError(\"Calculating L1 loss on 1D tensors? should not occur for this network\")\n","    \n","    @staticmethod\n","    def gram_matrix(x, norm_by_channels=False):\n","        \"\"\"Calculate gram matrix used in style loss\"\"\"\n","        \n","        # Assertions on input\n","        assert K.ndim(x) == 4, 'Input tensor should be a 4d (B, H, W, C) tensor'\n","        assert K.image_data_format() == 'channels_last', \"Please use channels-last format\"        \n","        \n","        # Permute channels and get resulting shape\n","        x = K.permute_dimensions(x, (0, 3, 1, 2))\n","        shape = K.shape(x)\n","        B, C, H, W = shape[0], shape[1], shape[2], shape[3]\n","        \n","        # Reshape x and do batch dot product\n","        features = K.reshape(x, K.stack([B, C, H*W]))\n","        gram = K.batch_dot(features, features, axes=2)\n","        \n","        # Normalize with channels, height and width\n","        gram = gram /  K.cast(C * H * W, x.dtype)\n","        \n","        return gram\n","    \n","    # Prediction functions\n","    ######################\n","    def predict(self, sample, **kwargs):\n","        \"\"\"Run prediction using this model\"\"\"\n","        return self.model.predict(sample, **kwargs)"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uv4u-5fNB9qC"},"source":["GENERADOR DE MASCARAS"]},{"cell_type":"code","metadata":{"id":"_ePoeGmiB_Rh","executionInfo":{"status":"ok","timestamp":1608210242219,"user_tz":300,"elapsed":2054,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}}},"source":["import os\n","from random import randint, seed\n","import itertools\n","import numpy as np\n","import cv2\n","\n","\n","class MaskGenerator():\n","\n","    def __init__(self, height, width, channels=3, rand_seed=None, filepath=None):\n","        \"\"\"Convenience functions for generating masks to be used for inpainting training\n","        \n","        Arguments:\n","            height {int} -- Mask height\n","            width {width} -- Mask width\n","        \n","        Keyword Arguments:\n","            channels {int} -- Channels to output (default: {3})\n","            rand_seed {[type]} -- Random seed (default: {None})\n","            filepath {[type]} -- Load masks from filepath. If None, generate masks with OpenCV (default: {None})\n","        \"\"\"\n","\n","        self.height = height\n","        self.width = width\n","        self.channels = channels\n","        self.filepath = filepath\n","\n","        # If filepath supplied, load the list of masks within the directory\n","        self.mask_files = []\n","        if self.filepath:\n","            filenames = [f for f in os.listdir(self.filepath)]\n","            self.mask_files = [f for f in filenames if any(filetype in f.lower() for filetype in ['.jpeg', '.png', '.jpg'])]\n","            print(\">> Found {} masks in {}\".format(len(self.mask_files), self.filepath))        \n","\n","        # Seed for reproducibility\n","        if rand_seed:\n","            seed(rand_seed)\n","\n","    def _generate_mask(self):\n","        \"\"\"Generates a random irregular mask with lines, circles and elipses\"\"\"\n","\n","        img = np.zeros((self.height, self.width, self.channels), np.uint8)\n","\n","        # Set size scale\n","        size = int((self.width + self.height) * 0.03)\n","        if self.width < 64 or self.height < 64:\n","            raise Exception(\"Width and Height of mask must be at least 64!\")\n","        \n","        # Draw random lines\n","        for _ in range(randint(1, 20)):\n","            x1, x2 = randint(1, self.width), randint(1, self.width)\n","            y1, y2 = randint(1, self.height), randint(1, self.height)\n","            thickness = randint(3, size)\n","            cv2.line(img,(x1,y1),(x2,y2),(1,1,1),thickness)\n","            \n","        # Draw random circles\n","        for _ in range(randint(1, 20)):\n","            x1, y1 = randint(1, self.width), randint(1, self.height)\n","            radius = randint(3, size)\n","            cv2.circle(img,(x1,y1),radius,(1,1,1), -1)\n","            \n","        # Draw random ellipses\n","        for _ in range(randint(1, 20)):\n","            x1, y1 = randint(1, self.width), randint(1, self.height)\n","            s1, s2 = randint(1, self.width), randint(1, self.height)\n","            a1, a2, a3 = randint(3, 180), randint(3, 180), randint(3, 180)\n","            thickness = randint(3, size)\n","            cv2.ellipse(img, (x1,y1), (s1,s2), a1, a2, a3,(1,1,1), thickness)\n","        \n","        return 1-img\n","\n","    def _load_mask(self, rotation=True, dilation=True, cropping=True):\n","        \"\"\"Loads a mask from disk, and optionally augments it\"\"\"\n","\n","        # Read image\n","        mask = cv2.imread(os.path.join(self.filepath, np.random.choice(self.mask_files, 1, replace=False)[0]))\n","        \n","        # Random rotation\n","        if rotation:\n","            rand = np.random.randint(-180, 180)\n","            M = cv2.getRotationMatrix2D((mask.shape[1]/2, mask.shape[0]/2), rand, 1.5)\n","            mask = cv2.warpAffine(mask, M, (mask.shape[1], mask.shape[0]))\n","            \n","        # Random dilation\n","        if dilation:\n","            rand = np.random.randint(5, 47)\n","            kernel = np.ones((rand, rand), np.uint8) \n","            mask = cv2.erode(mask, kernel, iterations=1)\n","            \n","        # Random cropping\n","        if cropping:\n","            x = np.random.randint(0, mask.shape[1] - self.width)\n","            y = np.random.randint(0, mask.shape[0] - self.height)\n","            mask = mask[y:y+self.height, x:x+self.width]\n","\n","        return (mask > 1).astype(np.uint8)\n","\n","    def sample(self, random_seed=None):\n","        \"\"\"Retrieve a random mask\"\"\"\n","        if random_seed:\n","            seed(random_seed)\n","        if self.filepath and len(self.mask_files) > 0:\n","            return self._load_mask()\n","        else:\n","            return self._generate_mask()\n","\n","\n","class ImageChunker(object): \n","    \n","    def __init__(self, rows, cols, overlap):\n","        self.rows = rows\n","        self.cols = cols\n","        self.overlap = overlap\n","    \n","    def perform_chunking(self, img_size, chunk_size):\n","        \"\"\"\n","        Given an image dimension img_size, return list of (start, stop) \n","        tuples to perform chunking of chunk_size\n","        \"\"\"\n","        chunks, i = [], 0\n","        while True:\n","            chunks.append((i*(chunk_size - self.overlap/2), i*(chunk_size - self.overlap/2)+chunk_size))\n","            i+=1\n","            if chunks[-1][1] > img_size:\n","                break\n","        n_count = len(chunks)        \n","        chunks[-1] = tuple(x - (n_count*chunk_size - img_size - (n_count-1)*self.overlap/2) for x in chunks[-1])\n","        chunks = [(int(x), int(y)) for x, y in chunks]\n","        return chunks\n","    \n","    def get_chunks(self, img, scale=1):\n","        \"\"\"\n","        Get width and height lists of (start, stop) tuples for chunking of img.\n","        \"\"\"\n","        x_chunks, y_chunks = [(0, self.rows)], [(0, self.cols)]        \n","        if img.shape[0] > self.rows:\n","            x_chunks = self.perform_chunking(img.shape[0], self.rows)\n","        else:\n","            x_chunks = [(0, img.shape[0])]\n","        if img.shape[1] > self.cols:\n","            y_chunks = self.perform_chunking(img.shape[1], self.cols)\n","        else:\n","            y_chunks = [(0, img.shape[1])]\n","        return x_chunks, y_chunks    \n","    \n","    def dimension_preprocess(self, img, padding=True):\n","        \"\"\"\n","        In case of prediction on image of different size than 512x512,\n","        this function is used to add padding and chunk up the image into pieces\n","        of 512x512, which can then later be reconstructed into the original image\n","        using the dimension_postprocess() function.\n","        \"\"\"\n","    \n","        # Assert single image input\n","        assert len(img.shape) == 3, \"Image dimension expected to be (H, W, C)\"\n","    \n","        # Check if we are adding padding for too small images\n","        if padding:\n","            \n","            # Check if height is too small\n","            if img.shape[0] < self.rows:\n","                padding = np.ones((self.rows - img.shape[0], img.shape[1], img.shape[2]))\n","                img = np.concatenate((img, padding), axis=0)\n","    \n","            # Check if width is too small\n","            if img.shape[1] < self.cols:\n","                padding = np.ones((img.shape[0], self.cols - img.shape[1], img.shape[2]))\n","                img = np.concatenate((img, padding), axis=1)\n","    \n","        # Get chunking of the image\n","        x_chunks, y_chunks = self.get_chunks(img)\n","    \n","        # Chunk up the image\n","        images = []\n","        for x in x_chunks:\n","            for y in y_chunks:\n","                images.append(\n","                    img[x[0]:x[1], y[0]:y[1], :]\n","                )\n","        images = np.array(images)        \n","        return images\n","    \n","    def dimension_postprocess(self, chunked_images, original_image, scale=1, padding=True):\n","        \"\"\"\n","        In case of prediction on image of different size than 512x512,\n","        the dimension_preprocess  function is used to add padding and chunk \n","        up the image into pieces of 512x512, and this function is used to \n","        reconstruct these pieces into the original image.\n","        \"\"\"\n","    \n","        # Assert input dimensions\n","        assert len(original_image.shape) == 3, \"Image dimension expected to be (H, W, C)\"\n","        assert len(chunked_images.shape) == 4, \"Chunked images dimension expected to be (B, H, W, C)\"\n","        \n","        # Check if we are adding padding for too small images\n","        if padding:\n","    \n","            # Check if height is too small\n","            if original_image.shape[0] < self.rows:\n","                new_images = []\n","                for img in chunked_images:\n","                    new_images.append(img[0:scale*original_image.shape[0], :, :])\n","                chunked_images = np.array(new_images)\n","    \n","            # Check if width is too small\n","            if original_image.shape[1] < self.cols:\n","                new_images = []\n","                for img in chunked_images:\n","                    new_images.append(img[:, 0:scale*original_image.shape[1], :])\n","                chunked_images = np.array(new_images)\n","            \n","        # Put reconstruction into this array\n","        new_shape = (\n","            original_image.shape[0]*scale,\n","            original_image.shape[1]*scale,\n","            original_image.shape[2]\n","        )\n","        reconstruction = np.zeros(new_shape)\n","            \n","        # Get the chunks for this image    \n","        x_chunks, y_chunks = self.get_chunks(original_image)\n","        \n","        i = 0\n","        s = scale\n","        for x in x_chunks:\n","            for y in y_chunks:\n","                \n","                prior_fill = reconstruction != 0\n","                chunk = np.zeros(new_shape)\n","                chunk[x[0]*s:x[1]*s, y[0]*s:y[1]*s, :] += chunked_images[i]\n","                chunk_fill = chunk != 0\n","                \n","                reconstruction += chunk\n","                reconstruction[prior_fill & chunk_fill] = reconstruction[prior_fill & chunk_fill] / 2\n","    \n","                i += 1\n","        \n","        return reconstruction"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3ArNzgucCDnM"},"source":["MAIN"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Jejx2vNlCEtp","executionInfo":{"status":"error","timestamp":1608214305961,"user_tz":300,"elapsed":73754,"user":{"displayName":"ANDY GIANPIERO ÑACA RODRIGUEZ","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8Xq_8LsflwwDQiVaawhmsbaENkFsyfwuW9l_N=s64","userId":"00235352279510428886"}},"outputId":"51254a14-7072-4c45-ee2f-858920e3fa46"},"source":["import os\n","import gc\n","import datetime\n","import numpy as np\n","import pandas as pd\n","import cv2\n","\n","from argparse import ArgumentParser\n","from copy import deepcopy\n","from tqdm import tqdm\n","\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.callbacks import TensorBoard, ModelCheckpoint, LambdaCallback\n","from keras import backend as K\n","from keras.utils import Sequence\n","from keras_tqdm import TQDMCallback\n","\n","import matplotlib.pyplot as plt\n","from matplotlib.ticker import NullFormatter\n","\n","\n","# Sample call\n","r\"\"\"\n","# Train on CelebaHQ\n","python main.py --name CelebHQ --train C:\\Documents\\Kaggle\\celebaHQ-512\\train\\ --validation C:\\Documents\\Kaggle\\celebaHQ-512\\val\\ --test C:\\Documents\\Kaggle\\celebaHQ-512\\test\\ --checkpoint \"C:\\Users\\Mathias Felix Gruber\\Documents\\GitHub\\PConv-Keras\\data\\logs\\imagenet_phase1_paperMasks\\weights.35-0.70.h5\"\n","\"\"\"\n","\n","os.chdir(\"/content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data\")\n","class parse_args():\n","    def __init__(self):\n","      self.stage='train' #or 'finetune'\n","      self.train='/content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train'\n","      self.validation='/content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/test'\n","      self.test='/content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/test'\n","      self.name='Div2K'\n","      self.batch_size=4\n","      self.test_path='./data/test_samples/'\n","      self.weight_path='./data/weigth/'\n","      self.log_path='./data/logs/'\n","      self.vgg_path=None\n","      self.checkpoint=None\n","    \n","    \"\"\"\n","    parser = ArgumentParser(description='Training script for PConv inpainting')\n","\n","    parser.add_argument(\n","        '-stage', '--stage',\n","        type=str, default='train',\n","        help='Which stage of training to run',\n","        choices=['train', 'finetune']\n","    )\n","\n","    parser.add_argument(\n","        '-train', '--train',\n","        type=str,\n","        help='Folder with training images'\n","    )\n","    \n","    parser.add_argument(\n","        '-validation', '--validation',\n","        type=str,\n","        help='Folder with validation images'\n","    )\n","\n","    parser.add_argument(\n","        '-test', '--test',\n","        type=str,\n","        help='Folder with testing images'\n","    )\n","        \n","    parser.add_argument(\n","        '-name', '--name',\n","        type=str, default='myDataset',\n","        help='Dataset name, e.g. \\'imagenet\\''\n","    )\n","        \n","    parser.add_argument(\n","        '-batch_size', '--batch_size',\n","        type=int, default=4,\n","        help='What batch-size should we use'\n","    )\n","\n","    parser.add_argument(\n","        '-test_path', '--test_path',\n","        type=str, default='./data/test_samples/',\n","        help='Where to output test images during training'\n","    )\n","        \n","    parser.add_argument(\n","        '-weight_path', '--weight_path',\n","        type=str, default='./data/logs/',\n","        help='Where to output weights during training'\n","    )\n","        \n","    parser.add_argument(\n","        '-log_path', '--log_path',\n","        type=str, default='./data/logs/',\n","        help='Where to output tensorboard logs during training'\n","    )\n","\n","    parser.add_argument(\n","        '-vgg_path', '--vgg_path',\n","        type=str, default='./data/logs/pytorch_to_keras_vgg16.h5',\n","        help='VGG16 weights trained on PyTorch with pixel scaling 1/255.'\n","    )\n","\n","    parser.add_argument(\n","        '-checkpoint', '--checkpoint',\n","        type=str, \n","        help='Previous weights to be loaded onto model'\n","    )\n","        \n","    return  parser.parse_args()\n","\"\"\"\n","\n","class AugmentingDataGenerator(ImageDataGenerator):\n","    \"\"\"Wrapper for ImageDataGenerator to return mask & image\"\"\"\n","    def flow_from_directory(self, directory, mask_generator, *args, **kwargs):\n","        generator = super().flow_from_directory(directory, class_mode=None, *args, **kwargs)      \n","        seed = None if 'seed' not in kwargs else kwargs['seed']\n","        while True:\n","            \n","            # Get augmentend image samples\n","            ori = next(generator)\n","            print(\"DIRECTORIO\",directory,ori.shape)\n","            # Get masks for each image sample            \n","            mask = np.stack([\n","                mask_generator.sample(seed)\n","                \n","                for _ in range(ori.shape[0])], axis=0\n","            )\n","\n","            # Apply masks to all image sample\n","            masked = deepcopy(ori)\n","            masked[mask==0] = 1\n","\n","            # Yield ([ori, masl],  ori) training batches\n","            # print(masked.shape, ori.shape)\n","            gc.collect()\n","            yield [masked, mask], ori\n","\n","# Run script\n","if __name__ == '__main__':\n","\n","    # Parse command-line arguments\n","    args = parse_args()\n","\n","    if args.stage == 'finetune' and not args.checkpoint:\n","        raise AttributeError('If you are finetuning your model, you must supply a checkpoint file')\n","\n","    # Create training generator\n","    train_datagen = AugmentingDataGenerator(  \n","        rotation_range=10,\n","        width_shift_range=0.1,\n","        height_shift_range=0.1,\n","        rescale=1./255,\n","        horizontal_flip=True\n","    )\n","    train_generator = train_datagen.flow_from_directory(\n","        args.train, \n","        MaskGenerator(512, 512, 3),\n","        target_size=(512, 512), \n","        batch_size=args.batch_size\n","    )\n","\n","    # Create validation generator\n","    val_datagen = AugmentingDataGenerator(rescale=1./255)\n","    val_generator = val_datagen.flow_from_directory(\n","        args.validation, \n","        MaskGenerator(512, 512, 3), \n","        target_size=(512, 512), \n","        batch_size=args.batch_size, \n","        classes=['val'], \n","        seed=42\n","    )\n","\n","    # Create testing generator\n","    test_datagen = AugmentingDataGenerator(rescale=1./255)\n","    test_generator = test_datagen.flow_from_directory(\n","        args.test, \n","        MaskGenerator(512, 512, 3), \n","        target_size=(512, 512), \n","        batch_size=args.batch_size, \n","        seed=42\n","    )\n","\n","    # Pick out an example to be send to test samples folder\n","    test_data = next(test_generator)\n","    (masked, mask), ori = test_data\n","\n","    def plot_callback(model, path):\n","        \"\"\"Called at the end of each epoch, displaying our previous test images,\n","        as well as their masked predictions and saving them to disk\"\"\"\n","        \n","        # Get samples & Display them        \n","        pred_img = model.predict([masked, mask])\n","        pred_time = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n","\n","        # Clear current output and display test images\n","        for i in range(len(ori)):\n","            _, axes = plt.subplots(1, 3, figsize=(20, 5))\n","            axes[0].imshow(masked[i,:,:,:])\n","            axes[1].imshow(pred_img[i,:,:,:] * 1.)\n","            axes[2].imshow(ori[i,:,:,:])\n","            axes[0].set_title('Masked Image')\n","            axes[1].set_title('Predicted Image')\n","            axes[2].set_title('Original Image')\n","                    \n","            plt.savefig(os.path.join(path, '/img_{}_{}.png'.format(i, pred_time)))\n","            plt.close()\n","\n","    # Load the model\n","    if args.vgg_path:\n","        model = PConvUnet(vgg_weights=args.vgg_path)\n","    else:\n","        model = PConvUnet()\n","    \n","    # Loading of checkpoint\n","    if args.checkpoint:\n","        if args.stage == 'train':\n","            model.load(args.checkpoint)\n","        elif args.stage == 'finetune':\n","            model.load(args.checkpoint, train_bn=False, lr=0.00005)\n","\n","    # Fit model\n","    model.fit_generator(\n","        train_generator, \n","        steps_per_epoch=10,\n","        validation_data=val_generator,\n","        validation_steps=10,\n","        epochs=10,  \n","        verbose=0,\n","        callbacks=[\n","            TensorBoard(\n","                log_dir=os.path.join(args.log_path, args.name+'_phase1'),\n","                write_graph=False\n","            ),\n","            ModelCheckpoint(\n","                os.path.join(args.log_path, args.name+'_phase1', 'weights.{epoch:02d}-{loss:.2f}.h5'),\n","                monitor='val_loss', \n","                save_best_only=True, \n","                save_weights_only=True\n","            ),\n","            LambdaCallback(\n","                on_epoch_end=lambda epoch, logs: plot_callback(model, args.test_path)\n","            ),\n","            TQDMCallback()\n","        ]\n","    )"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Found 509 images belonging to 1 classes.\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/test (4, 512, 512, 3)\n","tracking <tf.Variable 'p_conv2d_139/Variable:0' shape=(7, 7, 3, 64) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_140/Variable:0' shape=(5, 5, 64, 128) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_141/Variable:0' shape=(5, 5, 128, 256) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_142/Variable:0' shape=(3, 3, 256, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_143/Variable:0' shape=(3, 3, 512, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_144/Variable:0' shape=(3, 3, 512, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_145/Variable:0' shape=(3, 3, 512, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_146/Variable:0' shape=(3, 3, 512, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_147/Variable:0' shape=(3, 3, 1024, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_148/Variable:0' shape=(3, 3, 1024, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_149/Variable:0' shape=(3, 3, 1024, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_150/Variable:0' shape=(3, 3, 1024, 512) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_151/Variable:0' shape=(3, 3, 768, 256) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_152/Variable:0' shape=(3, 3, 384, 128) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_153/Variable:0' shape=(3, 3, 192, 64) dtype=float32> kernel_mask\n","tracking <tf.Variable 'p_conv2d_154/Variable:0' shape=(3, 3, 67, 3) dtype=float32> kernel_mask\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Training:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Found 0 images belonging to 1 classes.\n","Found 291 images belonging to 1 classes.\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/test (0, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","Epoch: 0:   0%|          | 0/10 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n","DIRECTORIO /content/drive/My Drive/UNSA/8 semestre/Proyecto fin de carrera/data/train (4, 512, 512, 3)\n"],"name":"stdout"},{"output_type":"error","ename":"ResourceExhaustedError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-af8fae9c548a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m                 \u001b[0mon_epoch_end\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mplot_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             ),\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0mTQDMCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         ]\n\u001b[1;32m    249\u001b[0m     )\n","\u001b[0;32m<ipython-input-8-3faa6b896bc3>\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, *args, **kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m         self.model.fit_generator(\n\u001b[1;32m    251\u001b[0m             \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    253\u001b[0m         )\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n","\u001b[0;32m/tensorflow-1.15.2/python3.6/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted: OOM when allocating tensor with shape[4,128,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node loss_18/outputs_img_loss/loss/model_18_2/vgg16/block2_conv2/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[Mean_18/_9563]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted: OOM when allocating tensor with shape[4,128,256,256] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node loss_18/outputs_img_loss/loss/model_18_2/vgg16/block2_conv2/convolution}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored."]}]}]}